% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/parquet.R
\name{parquet_large_csvs}
\alias{parquet_large_csvs}
\title{Convert large CSV files in the data pond to parquet format}
\usage{
parquet_large_csvs(
  data_pond = file.path("data_store", "data_pond"),
  data_parquet = file.path("data_store", "data_parquet"),
  views_dir = file.path("data_index", "views"),
  min_size_mb = 200,
  compression = "zstd",
  chunk_size = 1e+05
)
}
\arguments{
\item{data_pond}{Directory containing raw CSV files.}

\item{data_parquet}{Directory where parquet files will be written.}

\item{views_dir}{Directory containing view YAML files.}

\item{min_size_mb}{Minimum CSV file size (in MB) required for conversion.}

\item{compression}{Compression codec passed to Arrow.}

\item{chunk_size}{Number of rows per chunk when reading CSV files.}
}
\value{
Invisibly returns TRUE.
}
\description{
\strong{Maintenance utility.} Identifies large CSV files in the data pond and
converts them to parquet format for improved performance.

This function is intended for \strong{periodic optimization and maintenance}.
It is not required for normal data access and should not be used in
analytical scripts.
}
\details{
Parquet support is optional and requires the \pkg{arrow} package.
The parquet conversion strategy and thresholds may change over time.
}
