% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/parquet.R
\name{parquet_large_csvs}
\alias{parquet_large_csvs}
\title{Convert large CSV files in the pond to Parquet}
\usage{
parquet_large_csvs(
  data_pond = file.path("data_store", "data_pond"),
  data_parquet = file.path("data_store", "data_parquet"),
  views_dir = file.path("data_index", "views"),
  min_size_mb = 200,
  compression = "zstd",
  chunk_size = 1e+05
)
}
\arguments{
\item{data_pond}{Directory containing raw CSV files}

\item{data_parquet}{Directory to write Parquet files}

\item{views_dir}{Directory containing view definitions}

\item{min_size_mb}{Minimum CSV size (MB) to trigger conversion}

\item{compression}{Parquet compression codec}

\item{chunk_size}{Number of rows per read chunk}
}
\description{
Scans the data pond for large CSV files, converts them to Parquet
using chunked reads, and updates views to prefer the Parquet
representation when available.
}
